---
order: 10
---

# 基于ceph-deploy部署Ceph



## 1、存储基础

### 1.1传统存储

传统的存储类型：

- DAS设备：

SAS、SATA、SCSI、IDE、USB

无论是那种接口，都是存储设备驱动下的磁盘设备，而磁盘设备其实就是一种存储，这种存储是直接接入到主板总线上去的。

- NAS设备：NFS、gFS、FTP

几乎所有的网络存储设备基本上都是以文件系统样式进行使用，无法进一步格式化操作。

- SAN:

SCSI协议、FC SAN、iSCSI

基于SAN方式提供给客户端操作系统的是一种块设备接口，这些设备间主要通过SCS协议来完成正常的通信。SCSI的结构类似于TCP/IP协议，也有很多层，但是SCSI协议主要是用来进行存储数据操作的。既然是分层方式实现的，那就是说，有部分层可以被替代。比如将物理层基于FC方式来实现，就形成了FCSAN,如果基于以太网方式来传递数据，就形成了SCS模式。

### 1.2传统的存储方式问题

- 存储处理能力不足：

传统的IDE的IO值是100次/秒，SATA固态磁盘500次/秒，NVMe固态硬盘达到2000-4000次/秒。即使磁盘的IO能力再大数十倍，难道能够抗住网站访问高峰期数十万、数百万甚至上亿用户的同时访问么？这同时还要受到主机网络O能力的限制。

- 存储空间能力不足：

    单块磁盘容量再大，也无法满足用户的正常访问所需的数据容量限制

- 单点问题

    单主机存储数据存在SPOF(single point of failure) 问题。

## 2、Ceph简介和特性

Ceph是一个多版本存储系统，它把每一个待管理的数据流（例如一个文件）切分为一到多个固定大小的对象数并以其为原子单元完成数据存取。
对象数据的底层存储服务是由多个主机(host)组成的存储集群，该集群也被称之为RADOS(Reliable Automatic Distributed Object Store)存储集群，即可靠、自动化、分布式对象存储系统librados是RADOS存储集群的API,它支持C、C+、Java、Python、Ruby和PHP等编程语言。

Ceph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表，论文发表于2006年），并随后贡献给开源社区。

[官方地址](https:/ceph.com/en/)
[官方文档](https://docs.ceph.com/en/latest/)
[github地址](https://github.com/ceph/ceph)

Cph通过内部的crush机制，实时方式计算出一个文件应该存储到那个存储对象里面，从而实现快速查找对像的一种方式。

为什么Ceph这么火？

- 功能强大:Cph能够同时提供对象存储、块存储和文件系统存储三种存储服务的统一存储架构

- 可扩展性：Ceph得以摒弃了传统的集中式存储元数据寻址方案，通过Crush:算法的寻址操作，有相当强大的扩展性。

- 高可用性：Cph数据副本数量可以由管理员自行定义，并可以通过Crush算法指定副本的物理存储位置以分隔故隨域，支持数据强一致性的特性也使Ceph具有了高可靠性，可以忍受多种故障场景并自动尝试并行修复。

RadosGW、RBD和CephFS都是RADOS存储服务的客户端，它们把RADOS的存储服务接口(librados)分别从不同的角度做了进一步抽象，因而各自适用于不同的应用场景。
也就是说，cph将三种存储类型统一在一个平台中，从而实现了更强大的适用性。
![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024141332.png)

:::tip
LIBRADOS: 通过自编程方式实现数据的存储能力

RADOSGW: 通过标准的RESTFUL接口，提供一种云存储服务

RBD: 将Ceph提供的控制，模拟成一个个的独立块设备、当ceph环境部署完后，服务端就准备好了rbd接口

CFS: 通过一个标准的文件系统接口来进行数据的存储
:::

|类型|典型设备|优点|缺点|使用场景|
|----|-------|----|---|--------|
|cephfs|FTP、NFS服务器<br>为了克服块存储文件无法访问的问题，所以有了文件存储<br>在服务器上架设FTP与NFS服务，就是文件存储|1、造价低 <br> 2、方便文件共享 |1、读写速率低<br>2、传输速率慢|1、日志存储 <br> 2、有目录结构的文件存储|
|rdb|磁盘阵列，硬盘<br>主要是将裸磁盘空间映射给主机使|1、通过Raid与LVM等手段,对数据提供了保护<br>2、多块廉价的硬盘组合起时，光纤交换机，造价<br>3、多块磁盘组合出来的逻2主机之间无法共享数辑盘，提升读写效率。据|1、采用SAN架构组网时，光纤交换机，造价成本高<br>2、主机直接无法共享数据|1、docker容器、虚拟机磁盘存储<br>2日志存储<br>3文件存储|
|rgw|内置大容量硬盘的分布式服务器(swift,s3)<br>多台服务器内置大容是硬盘,安装上对像存储管理软件，对外提供读写功能|1、具备块存储的读写高速<br>2、具备文件存储的共享等||(适合更新变动较少的数据)<br>1、图片存储<br>2、视频存储|

## 3、Ceph组件

![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024144051.png)

无论是想向云平台提供Ceph对象存储和Ceph块设备服务、部署Ceph文件系统，或者是将Ceph用于其他目的，所有Ceph存储集群部署都从设置每个Ceph节点、网络开始。

一个Ceph存储集群至少需要-个`Ceph Monitor`、`Ceph Manager`和`Ceph OSD`(OBJECT STORAGE DAEMON对象存储守护进程)。此外如果有运行Ceph文件系统的客户端，还需要配置`Ceph`元数据服务器。

|组件|解析|
|----|----|
|Monitors|Ceph Monitor(守护进程ceph-mon)维护集群状态的映射，包括监视器映射、管理器映射，OSD映射、MDS映射和CRUSH映射。这些映射是Ceph守护进程相互协调所需的关键集群状|
|Managers|Ceph管理器（守护进程ceph-mgr)负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph管理器守护进程还托管基于Python的模块来管理和公开Ceph集群信息，包括基于Web的Ceph仪表板和REST API。高可用性通常至少需要两个管理器。基于raft协议实现节点间的信息同步。|
|Ceph OSDS|Ceph OSD(Object Storage Daemon对象存储守护进程ceph-osd)存储数据，处理数据复制、恢复、重新平衡，并通过检查其他Ceph OSD守护进程的心跳来向Ceph监视器和管理器提供一些监控信息。通常至少需要3个Ceph OSD来实现冗余和高可用性。本质上osd就是一个个host主机上的存储磁盘。|
|MDSs|Ceph元数据服务器(MDS[Metadata Server]、ceph-mds)代表Ceph文件系统存储元数据。Ceph元数据服务器允许POSX(为应用程序提供的接口标准)文件系统用户执行基本命令（如Is、find等)，而不会给Ceph存储集群带来巨大负担。|

## 4、Ceph网络模型

Ceph生产环境中一般分为两个网段

- 公有网络: 用于用户的数据通信

- 集群网络：用于集群内部的管理通信

![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024145318.png)

## 5、Cpeh版本

每个Ceph的版本都有一个英文的名称和一个数字形式的版本编号

第一个Ceph版本编号是0.1，发布于2008年1月。之后是0.2,0.3.多年来，版本号方案一直没变

2015年4月0.94.1(Hammer的第一个修正版)发布后，为了避免0.99（以及0.100或1.00），制定了新策略。

x将从9算起，它代表版本名称nfernalis(|是第九个字母)，这样第九个发布周期的第一个开发版就是9.0.0；后续的开发版依次是9.0.1、9.0.2等等。
:::tip
X0.2-开发版

×1.2-候选版

×22-稳定、修正版
[Ceph版本说明](https://docs.ceph.com/en/latest/releases/)
:::

## 6、Ceph部署方法介绍

由于Cpeh组件众多以及环境复杂、所以官方提供了多种的快速部署工具和方法。
[参考资料](https://docs.ceph.com/en/quincy/install/)
![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024150127.png)

|部署方式|特点解析|
|-------|------|
|Cephadm|使用容器和systemd安装和管理Ceph集群，并与CLL和仪表板GUI紧密集成。只支持Octopus以后的新版本，当前官方推荐|
|ceph-deploy|一个基于Python实现流行的快速部署集群的工具，此工具从Nautilus版后不再支持和测试，建议使用此工具安装Nautilus之前旧版本|
|Rook|在Kubernetes中运行的Ceph集群，同时还支持通过Kubernetes API管理存储资源和配置。只支持Nautilus以后的新版本，此工具不支持RHEL8,CentOS8较新的OS|
|ceph-ansible|使用Ansible部署和管理Ceph集群，应用较广。但是从Nautlius和Octopus版没有集成对应的APL,所以较新的管理功能和仪表板集成不可用。|
|ceph-salt|使用Salt和cephadm安装Ceph|
|ceph-mon|使用Juju(模型驱动的Kubernetes Operators生命令周期管理器QLM)安装Ceph|
|Puppet-ceph|通过Puppet安装Ceph|

## 7、Ceph环境规划

|主机名|公有网络|集群网络|系统|内核|角色|
|------|-------|-------|----|----|---|
|ceph01|192.168.248.101|192.168.1.101|centos 7.9||admin, mon,mgr,osd|
|ceph02|192.168.248.102|192.168.1.102|centos 7.9|| mon,mgr,osd|
|ceph03|192.168.248.103|192.168.1.103|centos 7.9|| mon,mgr,osd|

:::tip
版本的选择
[版本地址](https:/docs.ceph.com/en/latest/releases./)

版本特性：X.0.z(开发版)、×.1.z(候选版)、x.2.z(稳定、修正版)

选择版本：Pacific v16.2.z

:::

## 8、基于ceph-deploy部署Ceph

### 8.1、环境准备

#### 8.1.1 升级内核版本

三台主机升级到最新的内核版本

```shell
#配置elrepo源
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm

#查看最新版内核
yum --disablerepo="*" --enablerepo="elrepo-kernel" list available
#安装最新的内核版本
yum --enablerepo=elrepo-kernel install kernel-ml -y
# 设置系统默认内核
awk -F\' '$1=="menuentry " {print i++ " : " $2}' /etc/grub2.cfg
grub2-set-default 0 
sed -i 's/saved/0/' /etc/default/grub
#重新创建内核配置 
grub2-mkconfig -o /boot/grub2/grub.cfg
#重新启动
reboot
```

#### 8.1.2 配置主机名解析

修改`hosts`文件，做好主机解析功能

```shell
cat >> /etc/hosts <<EOF
192.168.1.101 ceph01
192.168.1.102 ceph02
192.168.1.103 ceph03
```

:::tip
主机名需要和host设置相同，必须设置否则无法初始化，后续也有问题！
:::

#### 8.1.3 同步服务器时间

配置时间同步服务,这里采用`chrony`时间软件
:::tip
centos 7.9 默认是已经安装`chrony`软件,如果没有就执行`yum -y install chroy`进行安装
:::
修改`chrony`配置文件,把`ceph01`主机的时间同步服务器指定为阿里云的，`ceph02`和`ceph02`的时间同步服务器指定为`ceph01`的

```shell
#ceph01配置如下
server ntp.aliyun.com iburst
server ntp1.aliyun.com iburst
server ntp2.aliyun.com iburst
allow 192.168.0.0/16
#ceph02和ceph03配置如下
server ceph01 iburst

#所以节点执行重启并设置开机启动
systemctl restart chronyd
systemctl enable chronyd
```

#### 8.1.4 关闭selinux

所有节点关闭selinux

```shell
setenforce 0
sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
```

#### 8.1.5 关闭防火墙

```shell
systemctl stop firewalld
systemctl disable firewalld

```

### 8.2、创建ceph管理用户

#### 8.2.1创建Ceph集群的管理用户cephadm用户

由于接下来的所有操作，基本上都在admin这个管理节点的主机上来运行，基于安全考虑不推荐直接使用root用户来管理，倾向于通过一个普通用户来操作接下来的操作。
由于后续的安装软件，涉及到root用户权限的操作，所以这个普通用户最好具备sudo的权限。注意：此管理用户不要使用ceph的名称，ceph后续会自动创建此用户方法1
在所有主机上都创建普通用户

```shell
useradd -m -s /bin/bash cephadm 
echo cephadm:123456 chpasswd
#为用户配置root权限
#修改/etc/sudoers文件，找到下面一行，在root下面添加一行，如下所示:
cephadm ALL = (root) NOPASSWD:ALL
```

并为cephadm用户配置免密登录，采用如下脚本：

```shell
ssh-keygen -t rsa -P "" -f /home/cephadm/.ssh/id_rsa
for i in {1..3};do ssh-copy-id  cephadm@ceph0$i;done
```

### 8.3、管理节点部署ceph的安装环境

#### 8.3.1 各节点配置yum源配置

该实验采用清华大学的yum源，配置如下：

```shell
#/etc/yum.repos.d/ceph.repo
[ceph]
name=ceph
baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7/x86_64/
gpgcheck=0

[ceph-noarch]
name=cephnoarch
baseurl=https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7/noarch/
gpgcheck=0
```

#### 8.3.2 admin节点主机安装ceph-deploy工具

ceph-deploy工具只需要安装在admin节点上，命令如下：

```shell
sudo yum -y install python-setuptools ceph-deploy 

```

验证成功和查看版本

```shell
[cephadm@ceph01 home]$ ceph-deploy --version
2.0.1
```

:::tip
安装ceph包的目的是为了以后能在ceph-admin节点中查看ceph集群的状态
:::

在`ceph01`节点上创建集群的工作目录，该目录会存放`ceph-deploy`指令行执时生成的日志和秘钥信息等：

```shell
mkdir -p /usr/local/ceph-cluster
```

### 8.4、集群初始化

查看初始化命令用法

```shell
ceph-deploy new --help
```

:::tip
初始化第-个Mon节点的命令格式为"ceph-deploy new{initial-monitor-node(s)}

- mon0l即为第一个Mon节点名称，其名称必须与节点当前实际使用的主机名称`uname-n`保存一致
:::

在`ceph01`节点上进入`/usr/local/ceph-cluster`目录，执行`ceph-deploy`命令初始化集群设置，初始化要求指定节点作为`mon`，命令如下

```shell

[cephadm@ceph01 ceph-cluster]$ sudo ceph-deploy new --public-network 192.168.248.0/24 --cluster-network 192.168.1.0/24 ceph01

```

:::tip

- public-network: 集群对外的网络

- cluster-network: 集群内通信的网络

:::

执行完上述命令，会在`ceph-cluster`目录生成若干个配置文件,如下：

```shell
[cephadm@ceph01 ceph-cluster]$ ls
ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
[cephadm@ceph01 ceph-cluster]$ cat ceph.conf
[global]
fsid = 124bbfb5-7eca-4121-8285-053d35de62dc
public_network = 192.168.248.0/24
cluster_network = 192.168.1.0/24
mon_initial_members = ceph01
mon_host = 192.168.248.128
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
```

### 8.5、Mon节点安装软件

使用`ceph-deploy`命令能够以远程的方式连入`Ceph`集群各节点完成程序包安装等操作以下安装mon节点的相关包,命令格式：

```shell
ceph-deploy install  {ceph-node}[{ceph-node}...
```

:::tip
这里主要是ceph的工作角色的的节点,一般情况下，不推荐使用这种直接的方法来进行安装，效率太低`ceph-deploy install mon01 mon02 mon03`
还有另外一种方法，手工在所有节点上安装ceph软件`install -y ceph ceph-osd ceph-mds ceph-mon radosgw`
:::

范例：由于在本案例中ceph01,充当的角色`ceph-deploy`、`mom`、`mgr`、`osd`,所以在ceph01节点安装这几个软件包

```shell
yum -y install ceph ceph-mon ceph-mgr ceph-mds ceph-radosgw
```

#### 8.5.1 初始化 mon

在初始化集群的过程中已经指定了`mon`的节点，现在需要对`mon`进行初始化，在`ceph01`节点的`/usr/local/ceph-cluster`目录下执行如下命令进行初始化：

```shell
[cephadm@ceph01 ceph-cluster]$ sudo ceph-deploy --overwrite-conf mon create-initial
```

初始化完成后，会在当前目录下生成各个组件需要的key文件：

```shell
[root@ceph01 ceph-cluster]# ll
总用量 80
-rw-------. 1 root root    71 10月 31 17:16 ceph.bootstrap-mds.keyring
-rw-------. 1 root root    71 10月 31 17:16 ceph.bootstrap-mgr.keyring
-rw-------. 1 root root    71 10月 31 17:16 ceph.bootstrap-osd.keyring
-rw-------. 1 root root    71 10月 31 17:16 ceph.bootstrap-rgw.keyring
-rw-------. 1 root root    63 10月 31 17:16 ceph.client.admin.keyring
-rw-r--r--. 1 root root   266 10月 31 17:16 ceph.conf
-rw-r--r--. 1 root root 46199 10月 31 17:16 ceph-deploy-ceph.log
-rw-------. 1 root root    77 10月 31 17:16 ceph.mon.keyring
-rw-------. 1 root root    73 10月 31 17:16 ceph.mon.keyring-20221031171630

```

### 8.6、在各节点上安装ceph相关软件包

由于本案例中三台机器中都充当`mon`、`mgr`和`osd`。所以全部机器上都要安装ceph的相关软件，执行命令如下:

```shell
sudo yum install -y ceph ceph-mon ceph-mgr ceph-radosgw ceph-mds
```

### 8.6、集群认证和管理

接下来将`admin`用户的`key`文件拷贝给各个`osd`节点，如果为了在`ceph01`节点中使用`ceph`命令查看集群状态，那么也需要将`key`文件拷贝给`ceph01`节点（`ceph1`节点需要安装`ceph`包）

```shell
[cephadm@ceph01 ceph-cluster]$ sudo ceph-deploy admin ceph01 ceph02 ceph03
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy admin ceph01 ceph02 ceph03
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f7fa1124368>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['ceph01', 'ceph02', 'ceph03']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f7fa118d230>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph01
[ceph01][DEBUG ] connected to host: ceph01
[ceph01][DEBUG ] detect platform information from remote host
[ceph01][DEBUG ] detect machine type
[ceph01][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph02
[ceph02][DEBUG ] connected to host: ceph02
[ceph02][DEBUG ] detect platform information from remote host
[ceph02][DEBUG ] detect machine type
[ceph02][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph03
[ceph03][DEBUG ] connected to host: ceph03
[ceph03][DEBUG ] detect platform information from remote host
[ceph03][DEBUG ] detect machine type
[ceph03][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
```

会在每个节点的`/etc/ceph/`目录下，生成若干个配置文件:

```shell
[cephadm@ceph02 ceph_rpm]$ cd /etc/ceph/
[cephadm@ceph02 ceph]$ ls
ceph.client.admin.keyring  ceph.conf  rbdmap  tmp89A2sV
```

拷贝完成后执行`ceph -s`命令可以查看到当前集群的状态：

```shell
[cephadm@ceph01 ceph]$ sudo ceph -s
  cluster:
    id:     124bbfb5-7eca-4121-8285-053d35de62dc
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum ceph01
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0B
    usage:   0B used, 0B / 0B avail
    pgs:
```

可以看到当前集群中已经有一个`mon`节点。

### 8.7、部署Mgr节点

:::tip
`Ceph MGR`工作的模式是事件驱动型的，简单来说，就是等待事件，事件来了则处理事件返回结果，又继续等待。
`Ceph MGR`事自从`Ceph12.2`依赖主推的功能之一，是负责Ceph集群管理的组件，它主要功能是把集群的一些指标暴露给外界使用。根据官方的架构原则上来说，mgr要有两个节点来进行工作。对于测试环境其实一个就能够正常使用了，暂时先安装一个节点，后面再安装第二个节点。
:::

配置`ceph01`节点作为`mgr`，在`ceph01`节点的`/usr/local/ceph-cluster`目录下执行如下命令：

```shell
[cephadm@ceph01 ceph-cluster]$ sudo ceph-deploy mgr create ceph01
```

查看集群状态，可以看到启动了一个`mgr daemon`：

```shell
[cephadm@ceph01 ceph]$ sudo ceph -s
  cluster:
    id:     124bbfb5-7eca-4121-8285-053d35de62dc
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum ceph01
    mgr: ceph01(active)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0B
    usage:   0B used, 0B / 0B avail
    pgs:
```

### 8.8、部署OSD存储节点

当前环境中，每个`OSD`节点都有一块未分配的空的`sdb`磁盘 ，信息如下：

```shell
[cephadm@ceph01 ceph]$ lsblk
NAME            MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sdb               8:16   0    5G  0 disk
sr0              11:0    1 1024M  0 rom
sda               8:0    0   20G  0 disk
├─sda2            8:2    0 19.5G  0 part
│ ├─centos-swap 253:1    0    2G  0 lvm  [SWAP]
│ ├─centos-home 253:2    0    5G  0 lvm  /home
│ └─centos-root 253:0    0 12.5G  0 lvm  /
└─sda1            8:1    0  200M  0 part /boot
```

接下来将每个`OSD`节点上的sdb磁盘加入到`ceph`集群中，命令如下：

```shell
for host in 01 02 03
do 
  sudo ceph-deploy disk zap ceph${host} /dev/sdb
  sudo ceph-deploy osd create ceph${host} --data /dev/sdb
done
```

:::tip

`ceph-deploy disk zap`命令用于将目标磁盘的分区表和内容擦除，实际上它是调用`/bin/dd 
if=/dev/zero of=/dev/sdb bs=1M count=10`，这里需要注意是磁盘必须为空的，没有分配的
`conv=fsync`命令来销毁`GPT`和`MBR`。如果目标磁盘是未被分区的，可以不使用该命令。
:::

执行完上面的命令sdb磁盘变成这样:

```shell
[cephadm@ceph01 ceph-cluster]$ lsblk  /dev/sdb
NAME                                                                                                  MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sdb                                                                                                     8:16   0   5G  0 disk
└─ceph--f396f2ce--9e3e--4bf0--8265--02d024430805-osd--block--7164fd94--5120--412c--bc82--1e18879fdd92 253:3    0   5G  0 lvm
```

此时查看`ceph`集群的状态，可以看到有三个`OSD`已经被加入到集群中：

```shell
[cephadm@ceph01 ceph-cluster]$ sudo ceph -s
  cluster:
    id:     124bbfb5-7eca-4121-8285-053d35de62dc
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum ceph01
    mgr: ceph01(active)
    osd: 3 osds: 2 up, 2 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0B
    usage:   2.01GiB used, 7.99GiB / 9.99GiB avail
    pgs:
```

### 8.9、扩展mon和mgr节点

> `mon`和`mgr`是`ceph`集群中非常重要的组件，其中`mon`作为整个集群的控制中心，里面存放着集群的信息，所以需要确保`mon`和`mgr`处于高可用的状态，为了保证选举正常，节点数要为奇数。

#### 8.9.1、扩容mon节点

首先将`ceph02`和`ceph03`扩容为`mon`节点：

```shell
sudo ceph-deploy mon add ceph02
sudo ceph-deploy mon add ceph03
```

扩容完成后查看集群状态:

```shell
[cephadm@ceph01 ceph-cluster]$ sudo ceph -s
  cluster:
    id:     7010fde7-be58-4867-947e-2b7aa3560ad8
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3
            clock skew detected on mon.ceph02, mon.ceph03

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0B
    usage:   0B used, 0B / 0B avail
    pgs:

```

#### 3.9.2、 扩容mgr节点

接下来扩容mgr节点，命令如下：

```shell
sudo ceph-deploy mgr create ceph02 ceph03
```

查看集群信息：

```shell
[cephadm@ceph01 ceph-cluster]$ sudo ceph -s
  cluster:
    id:     7010fde7-be58-4867-947e-2b7aa3560ad8
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3
            clock skew detected on mon.ceph02, mon.ceph03

  services:
    mon: 3 daemons, quorum ceph01,ceph02,ceph03
    mgr: ceph01(active), standbys: ceph03, ceph02
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0B
    usage:   0B used, 0B / 0B avail
    pgs:

```

## 参考文档

- [使用ceph-deploy 安装ceph](https://www.cnblogs.com/weiwei2021/p/14060186.html)

- [Ceph分布式集群安装配置](https://mp.weixin.qq.com/s/6XNBYZvHreQH8M2KZ2sb-Q)

- [Ceph-deploy 快速部署Ceph集群](https://i4t.com/5267.html) 
