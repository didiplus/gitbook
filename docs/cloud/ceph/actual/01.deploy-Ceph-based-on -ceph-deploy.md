---
order: 10
---

# 基于ceph-deploy部署Ceph



## 1、存储基础

### 1.1传统存储

传统的存储类型：

- DAS设备：

SAS、SATA、SCSI、IDE、USB

无论是那种接口，都是存储设备驱动下的磁盘设备，而磁盘设备其实就是一种存储，这种存储是直接接入到主板总线上去的。

- NAS设备：NFS、gFS、FTP

几乎所有的网络存储设备基本上都是以文件系统样式进行使用，无法进一步格式化操作。

- SAN:

SCSI协议、FC SAN、iSCSI

基于SAN方式提供给客户端操作系统的是一种块设备接口，这些设备间主要通过SCS协议来完成正常的通信。SCSI的结构类似于TCP/IP协议，也有很多层，但是SCSI协议主要是用来进行存储数据操作的。既然是分层方式实现的，那就是说，有部分层可以被替代。比如将物理层基于FC方式来实现，就形成了FCSAN,如果基于以太网方式来传递数据，就形成了SCS模式。

### 1.2传统的存储方式问题

- 存储处理能力不足：

传统的IDE的IO值是100次/秒，SATA固态磁盘500次/秒，NVMe固态硬盘达到2000-4000次/秒。即使磁盘的IO能力再大数十倍，难道能够抗住网站访问高峰期数十万、数百万甚至上亿用户的同时访问么？这同时还要受到主机网络O能力的限制。

- 存储空间能力不足：

    单块磁盘容量再大，也无法满足用户的正常访问所需的数据容量限制

- 单点问题

    单主机存储数据存在SPOF(single point of failure) 问题。

## 2、Ceph简介和特性

Ceph是一个多版本存储系统，它把每一个待管理的数据流（例如一个文件）切分为一到多个固定大小的对象数并以其为原子单元完成数据存取。
对象数据的底层存储服务是由多个主机(host)组成的存储集群，该集群也被称之为RADOS(Reliable Automatic Distributed Object Store)存储集群，即可靠、自动化、分布式对象存储系统librados是RADOS存储集群的API,它支持C、C+、Java、Python、Ruby和PHP等编程语言。

Ceph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表，论文发表于2006年），并随后贡献给开源社区。

[官方地址](https:/ceph.com/en/)
[官方文档](https://docs.ceph.com/en/latest/)
[github地址](https://github.com/ceph/ceph)

Cph通过内部的crush机制，实时方式计算出一个文件应该存储到那个存储对象里面，从而实现快速查找对像的一种方式。

为什么Ceph这么火？

- 功能强大:Cph能够同时提供对象存储、块存储和文件系统存储三种存储服务的统一存储架构

- 可扩展性：Ceph得以摒弃了传统的集中式存储元数据寻址方案，通过Crush:算法的寻址操作，有相当强大的扩展性。

- 高可用性：Cph数据副本数量可以由管理员自行定义，并可以通过Crush算法指定副本的物理存储位置以分隔故隨域，支持数据强一致性的特性也使Ceph具有了高可靠性，可以忍受多种故障场景并自动尝试并行修复。

RadosGW、RBD和CephFS都是RADOS存储服务的客户端，它们把RADOS的存储服务接口(librados)分别从不同的角度做了进一步抽象，因而各自适用于不同的应用场景。
也就是说，cph将三种存储类型统一在一个平台中，从而实现了更强大的适用性。
![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024141332.png)

:::tip
LIBRADOS: 通过自编程方式实现数据的存储能力

RADOSGW: 通过标准的RESTFUL接口，提供一种云存储服务

RBD: 将Ceph提供的控制，模拟成一个个的独立块设备、当ceph环境部署完后，服务端就准备好了rbd接口

CFS: 通过一个标准的文件系统接口来进行数据的存储
:::

|类型|典型设备|优点|缺点|使用场景|
|----|-------|----|---|--------|
|cephfs|FTP、NFS服务器<br>为了克服块存储文件无法访问的问题，所以有了文件存储<br>在服务器上架设FTP与NFS服务，就是文件存储|1、造价低 <br> 2、方便文件共享 |1、读写速率低<br>2、传输速率慢|1、日志存储 <br> 2、有目录结构的文件存储|
|rdb|磁盘阵列，硬盘<br>主要是将裸磁盘空间映射给主机使|1、通过Raid与LVM等手段,对数据提供了保护<br>2、多块廉价的硬盘组合起时，光纤交换机，造价<br>3、多块磁盘组合出来的逻2主机之间无法共享数辑盘，提升读写效率。据|1、采用SAN架构组网时，光纤交换机，造价成本高<br>2、主机直接无法共享数据|1、docker容器、虚拟机磁盘存储<br>2日志存储<br>3文件存储|
|rgw|内置大容量硬盘的分布式服务器(swift,s3)<br>多台服务器内置大容是硬盘,安装上对像存储管理软件，对外提供读写功能|1、具备块存储的读写高速<br>2、具备文件存储的共享等||(适合更新变动较少的数据)<br>1、图片存储<br>2、视频存储|

## 3、Ceph组件

![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024144051.png)

无论是想向云平台提供Ceph对象存储和Ceph块设备服务、部署Ceph文件系统，或者是将Ceph用于其他目的，所有Ceph存储集群部署都从设置每个Ceph节点、网络开始。

一个Ceph存储集群至少需要-个`Ceph Monitor`、`Ceph Manager`和`Ceph OSD`(OBJECT STORAGE DAEMON对象存储守护进程)。此外如果有运行Ceph文件系统的客户端，还需要配置`Ceph`元数据服务器。

|组件|解析|
|----|----|
|Monitors|Ceph Monitor(守护进程ceph-mon)维护集群状态的映射，包括监视器映射、管理器映射，OSD映射、MDS映射和CRUSH映射。这些映射是Ceph守护进程相互协调所需的关键集群状|
|Managers|Ceph管理器（守护进程ceph-mgr)负责跟踪运行时指标和Ceph集群的当前状态，包括存储利用率、当前性能指标和系统负载。Ceph管理器守护进程还托管基于Python的模块来管理和公开Ceph集群信息，包括基于Web的Ceph仪表板和REST API。高可用性通常至少需要两个管理器。基于raft协议实现节点间的信息同步。|
|Ceph OSDS|Ceph OSD(Object Storage Daemon对象存储守护进程ceph-osd)存储数据，处理数据复制、恢复、重新平衡，并通过检查其他Ceph OSD守护进程的心跳来向Ceph监视器和管理器提供一些监控信息。通常至少需要3个Ceph OSD来实现冗余和高可用性。本质上osd就是一个个host主机上的存储磁盘。|
|MDSs|Ceph元数据服务器(MDS[Metadata Server]、ceph-mds)代表Ceph文件系统存储元数据。Ceph元数据服务器允许POSX(为应用程序提供的接口标准)文件系统用户执行基本命令（如Is、find等)，而不会给Ceph存储集群带来巨大负担。|

## 4、Ceph网络模型

Ceph生产环境中一般分为两个网段

- 公有网络: 用于用户的数据通信

- 集群网络：用于集群内部的管理通信

![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024145318.png)

## 5、Cpeh版本

每个Ceph的版本都有一个英文的名称和一个数字形式的版本编号

第一个Ceph版本编号是0.1，发布于2008年1月。之后是0.2,0.3.多年来，版本号方案一直没变

2015年4月0.94.1(Hammer的第一个修正版)发布后，为了避免0.99（以及0.100或1.00），制定了新策略。

x将从9算起，它代表版本名称nfernalis(|是第九个字母)，这样第九个发布周期的第一个开发版就是9.0.0；后续的开发版依次是9.0.1、9.0.2等等。
:::tip
X0.2-开发版

×1.2-候选版

×22-稳定、修正版
[Ceph版本说明](https://docs.ceph.com/en/latest/releases/)
:::

## 6、Ceph部署方法介绍

由于Cpeh组件众多以及环境复杂、所以官方提供了多种的快速部署工具和方法。
[参考资料](https://docs.ceph.com/en/quincy/install/)
![](https://didiplus.oss-cn-hangzhou.aliyuncs.com/blog/20221024150127.png)

|部署方式|特点解析|
|-------|------|
|Cephadm|使用容器和systemd安装和管理Ceph集群，并与CLL和仪表板GUI紧密集成。只支持Octopus以后的新版本，当前官方推荐|
|ceph-deploy|一个基于Python实现流行的快速部署集群的工具，此工具从Nautilus版后不再支持和测试，建议使用此工具安装Nautilus之前旧版本|
|Rook|在Kubernetes中运行的Ceph集群，同时还支持通过Kubernetes API管理存储资源和配置。只支持Nautilus以后的新版本，此工具不支持RHEL8,CentOS8较新的OS|
|ceph-ansible|使用Ansible部署和管理Ceph集群，应用较广。但是从Nautlius和Octopus版没有集成对应的APL,所以较新的管理功能和仪表板集成不可用。|
|ceph-salt|使用Salt和cephadm安装Ceph|
|ceph-mon|使用Juju(模型驱动的Kubernetes Operators生命令周期管理器QLM)安装Ceph|
|Puppet-ceph|通过Puppet安装Ceph|

## 7、Ceph环境规划

|主机名|公有网络|集群网络|系统|内核|角色|
|------|-------|-------|----|----|---|
|ceph01|192.168.248.101|192.168.1.101|centos 7.9||ceph-deploy, mon,mgr,osd|
|ceph02|192.168.248.102|192.168.1.102|centos 7.9|| mon,mgr,osd|
|ceph03|192.168.248.103|192.168.1.103|centos 7.9|| mon,mgr,osd|

:::tip
版本的选择
[版本地址](https:/docs.ceph.com/en/latest/releases./)

版本特性：X.0.z(开发版)、×.1.z(候选版)、x.2.z(稳定、修正版)

选择版本：Pacific v16.2.z

:::

## 8、基于ceph-deploy部署Ceph

### 8.1、环境准备

三台主机升级到最新的内核版本

```shell
#配置elrepo源
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm

#查看最新版内核
yum --disablerepo="*" --enablerepo="elrepo-kernel" list available
#安装最新的内核版本
yum --enablerepo=elrepo-kernel install kernel-ml -y
# 设置系统默认内核
awk -F\' '$1=="menuentry " {print i++ " : " $2}' /etc/grub2.cfg
grub2-set-default 0 
sed -i 's/saved/0/' /etc/default/grub
#重新创建内核配置 
grub2-mkconfig -o /boot/grub2/grub.cfg
#重新启动
```

修改`hosts`文件，做好主机解析功能

```shell
cat >> /etc/hosts <<EOF
192.168.1.101 ceph01
192.168.1.102 ceph02
192.168.1.103 ceph02
```

:::tip
主机名需要和host设置相同，必须设置否则无法初始化，后续也有问题！
:::

配置时间同步服务,这里采用`chrony`时间软件
:::tip
centos 7.9 默认是已经安装`chrony`软件,如果没有就执行`yum -y install chroy`进行安装
:::
修改`chrony`配置文件,把`ceph01`主机的时间同步服务器指定为阿里云的，`ceph02`和`ceph02`的时间同步服务器指定为`ceph01`的

```shell
#ceph01配置如下
server ntp.aliyun.com iburst
server ntp1.aliyun.com iburst
server ntp2.aliyun.com iburst
allow 192.168.0.0/16
#ceph02和ceph03配置如下
server ceph01 iburst

#所以节点执行重启并设置开机启动
systemctl restart chronyd
systemctl enable chronyd
```

所有节点关闭防火墙selinux

```shell
systemctl stop firewalld
systemctl disable firewalld

setenforce 0
sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
```

### 8.2、创建ceph管理用户

#### 8.2.1创建Ceph集群的管理用户cephadm用户
由于接下来的所有操作，基本上都在admin这个管理节点的主机上来运行，基于安全考虑不推荐直接使用root用户来管理，倾向于通过一个普通用户来操作接下来的操作。
由于后续的安装软件，涉及到root用户权限的操作，所以这个普通用户最好具备sudo的权限。注意：此管理用户不要使用ceph的名称，ceph后续会自动创建此用户方法1
在所有主机上都创建普通用户
```
useradd -m -s /bin/bash cephadmecho cephadm:123456 chpasswd
#为用户配置root权限
echo "cephadm ALL (root)NOPASSWD:ALL">/etc/sudoers.d/cephadm
chmod 0440 /etc/sudoers.d/cephadm
```

### 8.3、管理节点部署ceph的安装环境


### 8.4、集群初始化

### 8.5、Mon节点安装软件

### 8.6、集群认证和管理

### 8.7、部署Mgr节点

### 8.8、部署OSD存储节点