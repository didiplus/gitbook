---
order: 1
---


# Ceph基础知识和基础架构

## Ceph概述

这里简单的说一下相关的组件，只是简单介绍

|组件       |概念    |
|-----------|------- |
|`Monitor`| 一个`Ceph`集群需要多个`Monitor`组成的小集群，它们通过`Paxos`同步数据，用来保存`OSD`的元数据 |
|`OSD`|`OSD`负责相应客户端请求返回具体数据的进程，一个`Ceph`集群一般都有很多个`OSD`|
|`MSD`|`MDS`全称`Cepg Metadata Service`，是`CephFs`服务依赖的元数据服务|
|`Object`|`Ceph`最底层的存储单位是`Object`对象，每个`Object`包含元数据和原始数据|
|`PG`|`PG`全称`Placement Groups`，是一个逻辑的概念，一个`PG`包含多个`OSD`。引入`PG`这一层其实是为了更好的分配数据和定位数据|
|`RADOS`|是`Ceph`集群的精华,为用户实现数据分配，`Failover`等集群操作|
|`Libradio`|`Libradio`是`Rados`提供库，因为`RADOS`是协议很难直接访问，因此上层的`RBD`、`RGW`和`CephFs`都是通过`librados`访问的目前提供`PHP`、`Ruby`、`Java`、`Python`等支持|
|`CRUSH`|`CRUSH`是`Ceph`使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。|
|`RBD`|`RBD`全称`RADOS block device`，是`Ceph`对外提供的块设备服务|
|`Image`|`RBD imag`e是简单的块设备，可以直接被`mount`到主机，成为一个`device`，用户可以直接写入二进制数据。`image`的数据被保存为若干个`RADOS`对象存储中的对象；`image`的数据空间是`thin provision`的，意味着`Ceph`不预分配空间，而是等到实际写入数据时按照`object`分配空间；每个`data object`被保存为多份。`pool`将`RBD`镜像的`ID`和`name`等基本信息保存在`rbd_directory`中，这样`rbd ls`命令就可以快速返回一个pool中所有的`RBD`镜像了更多`Image`信息|
|`RGW`|`RGW`全称`RADOS gateway`，是`Ceph`对外提供的对象存储服务，接口与`S3`和`Swift`兼容|
|`CephFs`|`CephFs`全称`Ceph File System`，是`Ceph`对外提供的文件系统服务|
|`pool`| `pool`是`Ceph`存储时的逻辑分区，它起到`namespace`的作用|

## Ceph介绍

- `ceph`是一个`Linux PB`级分布式文件系统，`Linux`持续不断进军可扩展计算空间，特别是可扩展存储空间。`Ceph`加入到`Linux`中令人印象深刻的文件系统备选行列，它是一个分布式文件系统，能够在维护`POSIX`兼容性的同时加入了复制和容错功能。
- `ceph`可以提供对象存储`RODOSGW`、块存储`RBD`、文件系统存储`Ceph FS`三种功能，以此满足不同场景的应用需求.

自下向上，可以将Ceph系统分为四个层次

1、基础存储系统`RADOS`

这一层本身就是一个完整的对象对象存储系统，所有存储在`Ceph`系统中的用户数据事实上最终都是由这一层来存储的。而`Ceph`高可靠、高可扩展、高性能、高自动化等等特性本质上也是由这一层所提供的。因此，理解`RADOS`是理解`Ceph`的关键与基础

2、基础库`librados`
这一层的功能是对`RADOS`进行抽象和封装，并向上层提供`API`，以便直接基于`RADOS`(而不是整个Ceph)进行应用开发。
:::tip
注意的是RADOS是一个对象存储系统。因此，librados实现的API也只是针对对象存储功能的
:::
`RADOS`采用`C++`开发，所提供的原生`librados API`包括`C`和`C++`两种，物理上，`librados`和基于其上开发的应用位于同一台机器，因而也被成为本地`API`。应用调用本机上的`librados API`，再由后者通过`socket`与`RADOS`集群中的节点通信并完成各种操作。

3、高层应用接口

这一层包括了三个部分：`RADOS (GWRADOS Gateway)` 、`RBD (Reliable Block Device)`和`Ceph FS (Ceph File System)`其作用是在`librados`库的基础上提供抽象层次更高的，更便于应用或客户端使用的上层接口

其中,`RADOS GW`是一个提供`Amazon S3`和`Swift`(内置大容量硬盘的分布式服务器)兼容的`RESTful API`的`Gateway`，以提供相应的对象存储应用开发使用。`RADOS GW`提供的`API`抽象层次更好，但功能则不如`librados`强大，应为开发者针对自己的需求选择使用

`RBD`则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建`Volume`。目前，`Red Hat`已经将`RBD`驱动集成在`KVM/QEMU`中，以提供虚拟机访问性能

`Ceph FS`是一个`POSIX`兼容的分布式文件系统,`Ceph`提供了`POSIX`接口，用户可以直接通过客户端挂载使用。它是内核态的程序，所以无需调用用户空间的`librados`库。通过内核中的net模块来和Rados进行交互

4、应用层

这一层就是不通场景下对Ceph各个应用接口的各种应用方式，例如基于`librados`直接开发的对象存储应用，基于`RADOCS GW`开发的对象存储应用，基于`RBD`实现的云硬盘等等。

## Ceph 基本组件

### 1、OSD

用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再均衡。并向其他`OSD`守护进程发送心跳，然后向`Mon`提供一些监控信息。
当`Ceph`存储集群设定数据有两个副本时(一共存2份),则至少需要两个`OSD`守护进程即两个`OSD`节点，集群才能达到`active+clean`状态

### 2、MDS

为`Ceph`文件提供元数据计算、缓存与同步。在`Ceph`中，元数据也是存储在`osd`节点中的，`osd`类似于元数据的代理缓存服务器。`mds`进程并不是必须的进程，只有使用`Cephfs`时，才需要配置`mds`

### 3、 Monitior

监控整个集群的状态，维护集群的`cluster MAP`二进制表，保证集群数据的一致性。`Cluster MAP`描述了对象存储的物理位置，以及将一个设备聚合到服务位置的桶列表。

## 二、三种存储类型

### 2.1、块存储rbd

典型设备：磁盘阵列，硬盘。主要是将裸磁盘空间映射给主机使用的

#### 优点

- 通过Raid与LVM等手段，对数据提供了保护
- 多块廉价的硬盘组合起来，提高容量
- 多块磁盘组合出来的逻辑盘，提高读写效率

#### 缺点

- 采用SAN架构组网时，光纤交换机，造价成本高
- 主机之间无法共享数据

#### 使用场景

- docker容器、虚拟机磁盘存储分配
- 日志存储
- 文件存储

### 2.2、文件存储fs

典型设备：FTP、NFS服务器。为了克服块存储文件无法共享的问题，所以有了文件存储。在服务器上架设FTP与NFS服务，就是文件存储

#### 优点

- 造价低，随便一台机器就可以了
- 方便文件共享

#### 缺点

- 读写速度低
- 传输速度慢

#### 使用场景

- 日志存储
- 有目录结构的文件存储

### 2.3、对象存储rgw

典型设备：内置大容量硬盘的分布式服务器(swift、s3)
多台服务器内置大容量硬盘，安装上对象存储管理软件，对外提供读写访问功能。

#### 优点

- 具备块存储的读写高速
- 具备文件存储的共享等特性

#### 使用场景 (适合更新变动比较少的数据)

- 图片存储
- 视频存储
